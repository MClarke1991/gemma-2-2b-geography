{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results/'\n",
    "fig_dir = os.path.join('figures/')\n",
    "pca_dir = os.path.join(fig_dir, 'overall_pca_and_classifier/')\n",
    "default_width = 1000\n",
    "default_height = 750\n",
    "\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "\n",
    "if not os.path.exists(pca_dir):\n",
    "    os.makedirs(pca_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(results_dir, 'transformed_distance_query_results_root_Los_Angeles-_CA.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_confident_column(df):\n",
    "    def classify_distance(distance):\n",
    "        # Convert to string to handle both float and int\n",
    "        distance_str = str(distance)\n",
    "        \n",
    "        # Check if the distance ends with two zeros before the decimal point\n",
    "        if re.search(r'00\\.?0*$', distance_str):\n",
    "            return 'guess'\n",
    "        else:\n",
    "            return 'confident'\n",
    "    \n",
    "    df['confidence'] = df['extracted_digits'].apply(classify_distance)\n",
    "    return df\n",
    "    \n",
    "data = add_confident_column(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot so that each index has its own column, fill na with zero\n",
    "data_pivot = data.pivot_table(index=['city2', 'sae_layer', 'feature_type', 'confidence']\n",
    "                               , columns='index'\n",
    "                               , values='activation'\n",
    "                               , fill_value=0.0)\n",
    "data_pivot = data_pivot.rename_axis(None, axis = 1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only the columns that exist\n",
    "columns_to_drop = ['city1', 'city2', 'extracted_digits']\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in data_pivot.columns]\n",
    "\n",
    "pca_data = data_pivot.drop(columns=existing_columns_to_drop)\n",
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Function to perform PCA and save plot\n",
    "# def perform_pca(X, confidence, sae_layer, feature_type, pca_dir):\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "#     pca = PCA()\n",
    "#     X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "#     loadings = pd.DataFrame(\n",
    "#         pca.components_.T,\n",
    "#         columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "#         index=X.columns\n",
    "#     )\n",
    "    \n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=confidence.map({'guess': 0, 'confident': 1}), \n",
    "#                           cmap='coolwarm', alpha=0.7)\n",
    "#     plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "#     plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "#     plt.title(f'PCA of Data Colored by Confidence\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "#     plt.colorbar(scatter, label='Confidence (0: Guess, 1: Confident)')\n",
    "    \n",
    "#     for i, (x, y) in enumerate(zip(loadings['PC1'], loadings['PC2'])):\n",
    "#         plt.arrow(0, 0, x, y, color='k', alpha=0.5, head_width=0.05, head_length=0.05)\n",
    "#         plt.text(x*1.2, y*1.2, loadings.index[i], color='g', ha='center', va='center')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     os.makedirs(os.path.join(pca_dir, f'pca_plots/{feature_type}'), exist_ok=True)\n",
    "#     plt.savefig(os.path.join(pca_dir, f'pca_plots/{feature_type}/pca_plot_layer{sae_layer}.png'), dpi = 300)\n",
    "#     plt.close()\n",
    "    \n",
    "#     return loadings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def perform_pca(X, confidence, sae_layer, feature_type, pca_dir):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "        index=X.columns\n",
    "    )\n",
    "    \n",
    "    # Get the top 5 loadings for PC1 and PC2\n",
    "    top_loadings = pd.concat([\n",
    "        loadings['PC1'].abs().nlargest(5),\n",
    "        loadings['PC2'].abs().nlargest(5)\n",
    "    ]).index.unique()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=confidence.map({'guess': 0, 'confident': 1}), \n",
    "                          cmap='coolwarm', alpha=0.7)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "    plt.title(f'PCA of Data Colored by Confidence\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "    plt.colorbar(scatter, label='Confidence (0: Guess, 1: Confident)')\n",
    "    \n",
    "    scale_factor = 5\n",
    "\n",
    "    # Plot only the top 5 loadings\n",
    "    for feature in top_loadings:\n",
    "        x, y = loadings.loc[feature, 'PC1'], loadings.loc[feature, 'PC2']\n",
    "        plt.arrow(0, 0, x*scale_factor, y*scale_factor, color='k', alpha=0.5, head_width=0.05, head_length=0.05)\n",
    "        plt.text(x*1.2*scale_factor, y*1.2*scale_factor, feature, color='g', ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(pca_dir, f'pca_plots/{feature_type}'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(pca_dir, f'pca_plots/{feature_type}/pca_plot_layer{sae_layer}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return loadings\n",
    "\n",
    "# Function to train classifier and get feature importances\n",
    "def train_classifier(X, y, sae_layer, feature_type, pca_dir):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = rf_classifier.predict(X_test_scaled)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names=['guess', 'confident'], output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    importances = rf_classifier.feature_importances_\n",
    "    feature_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "    \n",
    "    # Plot only top 10 feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_importances.head(10).plot(kind='bar')\n",
    "    plt.title(f'Top 10 Feature Importances\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(pca_dir, f'classifier_plots/{feature_type}'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(pca_dir, f'classifier_plots/{feature_type}/feature_importance_layer{sae_layer}.png'), dpi = 300)\n",
    "    plt.close()\n",
    "    \n",
    "    return report, conf_matrix, feature_importances\n",
    "\n",
    "def create_activation_heatmap(X, confidence, sae_layer, feature_type, pca_dir, feature_importances):\n",
    "    # Get the top 10 features\n",
    "    top_10_features = feature_importances.nlargest(10).index\n",
    "\n",
    "    # Filter X to include only top 10 features\n",
    "    X_top10 = X[top_10_features]\n",
    "\n",
    "    # Calculate average activation for guess and confident\n",
    "    avg_activation = X_top10.groupby(confidence).mean().T\n",
    "\n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(avg_activation, cmap='Blues', annot=True, fmt='.2f')\n",
    "    plt.title(f'Average Activation of Top 10 Features\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Top 10 Features')\n",
    "\n",
    "    # Save heatmap\n",
    "    os.makedirs(os.path.join(pca_dir, f'activation_heatmaps/{feature_type}'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(pca_dir, f'activation_heatmaps/{feature_type}/activation_heatmap_layer{sae_layer}.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return avg_activation\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(pca_dir, 'pca_plots'), exist_ok=True)\n",
    "os.makedirs(os.path.join(pca_dir, 'classifier_plots'), exist_ok=True)\n",
    "os.makedirs(os.path.join(pca_dir, 'text_results'), exist_ok=True)\n",
    "os.makedirs(os.path.join(pca_dir, 'activation_heatmaps'), exist_ok=True)\n",
    "\n",
    "# Group by sae_layer and feature_type\n",
    "grouped = pca_data.groupby(['sae_layer', 'feature_type'])\n",
    "\n",
    "# Perform analysis for each group\n",
    "for (sae_layer, feature_type), group in grouped:\n",
    "    print(f\"Processing Layer: {sae_layer}, Type: {feature_type}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = group.drop(['confidence', 'sae_layer', 'feature_type'], axis=1)\n",
    "    confidence = group['confidence']\n",
    "    y = confidence.map({'guess': 0, 'confident': 1})\n",
    "    \n",
    "    # Perform PCA\n",
    "    loadings = perform_pca(X, confidence, sae_layer, feature_type, pca_dir)\n",
    "    \n",
    "    # Train classifier\n",
    "    report, conf_matrix, feature_importances = train_classifier(X, y, sae_layer, feature_type, pca_dir)\n",
    "    \n",
    "    # Create activation heatmap\n",
    "    avg_activation = create_activation_heatmap(X, confidence, sae_layer, feature_type, pca_dir, feature_importances)\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(os.path.join(pca_dir, f'text_results/{feature_type}'), exist_ok=True)\n",
    "    with open(os.path.join(pca_dir, f'text_results/{feature_type}/results_layer{sae_layer}.txt'), 'w') as f:\n",
    "        f.write(f\"Results for Layer: {sae_layer}, Type: {feature_type}\\n\\n\")\n",
    "        f.write(\"PCA Loadings (top 10):\\n\")\n",
    "        f.write(loadings['PC1'].abs().sort_values(ascending=False).head(10).to_string())\n",
    "        f.write(\"\\n\\nClassification Report:\\n\")\n",
    "        f.write(pd.DataFrame(report).transpose().to_string())\n",
    "        f.write(\"\\n\\nConfusion Matrix:\\n\")\n",
    "        f.write(str(conf_matrix))\n",
    "        f.write(\"\\n\\nTop 10 Feature Importances:\\n\")\n",
    "        f.write(feature_importances.head(10).to_string())\n",
    "        f.write(\"\\n\\nAverage Feature Activation:\\n\")\n",
    "        f.write(avg_activation.to_string())\n",
    "\n",
    "print(\"Analysis complete. Results saved in the 'output' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "\n",
    "# # Function to perform PCA and save plot\n",
    "# # def perform_pca(X, confidence, sae_layer, feature_type, pca_dir):\n",
    "# #     scaler = StandardScaler()\n",
    "# #     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "# #     pca = PCA()\n",
    "# #     X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "# #     loadings = pd.DataFrame(\n",
    "# #         pca.components_.T,\n",
    "# #         columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "# #         index=X.columns\n",
    "# #     )\n",
    "    \n",
    "# #     plt.figure(figsize=(10, 8))\n",
    "# #     scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=confidence.map({'guess': 0, 'confident': 1}), \n",
    "# #                           cmap='coolwarm', alpha=0.7)\n",
    "# #     plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "# #     plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "# #     plt.title(f'PCA of Data Colored by Confidence\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "# #     plt.colorbar(scatter, label='Confidence (0: Guess, 1: Confident)')\n",
    "    \n",
    "# #     for i, (x, y) in enumerate(zip(loadings['PC1'], loadings['PC2'])):\n",
    "# #         plt.arrow(0, 0, x, y, color='k', alpha=0.5, head_width=0.05, head_length=0.05)\n",
    "# #         plt.text(x*1.2, y*1.2, loadings.index[i], color='g', ha='center', va='center')\n",
    "    \n",
    "# #     plt.tight_layout()\n",
    "# #     os.makedirs(os.path.join(pca_dir, f'pca_plots/{feature_type}'), exist_ok=True)\n",
    "# #     plt.savefig(os.path.join(pca_dir, f'pca_plots/{feature_type}/pca_plot_layer{sae_layer}.png'), dpi = 300)\n",
    "# #     plt.close()\n",
    "    \n",
    "# #     return loadings\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def perform_pca(X, confidence, sae_layer, feature_type, pca_dir):\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "#     pca = PCA()\n",
    "#     X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "#     loadings = pd.DataFrame(\n",
    "#         pca.components_.T,\n",
    "#         columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "#         index=X.columns\n",
    "#     )\n",
    "    \n",
    "#     # Get the top 5 loadings for PC1 and PC2\n",
    "#     top_loadings = pd.concat([\n",
    "#         loadings['PC1'].abs().nlargest(5),\n",
    "#         loadings['PC2'].abs().nlargest(5)\n",
    "#     ]).index.unique()\n",
    "    \n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=confidence.map({'guess': 0, 'confident': 1}), \n",
    "#                           cmap='coolwarm', alpha=0.7)\n",
    "#     plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "#     plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "#     plt.title(f'PCA of Data Colored by Confidence\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "#     plt.colorbar(scatter, label='Confidence (0: Guess, 1: Confident)')\n",
    "    \n",
    "#     scale_factor = 5\n",
    "\n",
    "#     # Plot only the top 5 loadings\n",
    "#     for feature in top_loadings:\n",
    "#         x, y = loadings.loc[feature, 'PC1'], loadings.loc[feature, 'PC2']\n",
    "#         plt.arrow(0, 0, x*scale_factor, y*scale_factor, color='k', alpha=0.5, head_width=0.05, head_length=0.05)\n",
    "#         plt.text(x*1.2*scale_factor, y*1.2*scale_factor, feature, color='g', ha='center', va='center')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     os.makedirs(os.path.join(pca_dir, f'pca_plots/{feature_type}'), exist_ok=True)\n",
    "#     plt.savefig(os.path.join(pca_dir, f'pca_plots/{feature_type}/pca_plot_layer{sae_layer}.png'))\n",
    "#     plt.close()\n",
    "    \n",
    "#     return loadings\n",
    "\n",
    "# def create_feature_effect_plot(model, X, feature_names, sae_layer, feature_type, pca_dir):\n",
    "#     try:\n",
    "#         # Get feature importances\n",
    "#         importances = model.feature_importances_\n",
    "#         indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#         # Ensure feature_names are strings and select top 5 features\n",
    "#         feature_names = [str(name) for name in feature_names]\n",
    "#         top_features = [feature_names[i] for i in indices[:5]]\n",
    "\n",
    "#         # Create partial dependence plot\n",
    "#         fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#         display = PartialDependenceDisplay.from_estimator(\n",
    "#             model, X, features=top_features,  # Use feature names instead of indices\n",
    "#             kind=\"average\", centered=True, \n",
    "#             subsample=1000, n_jobs=3, \n",
    "#             grid_resolution=20, random_state=42,\n",
    "#             ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
    "#             pd_line_kw={\"color\": \"tab:orange\", \"linewidth\": 2},\n",
    "#         )\n",
    "#         display.plot(ax=ax)\n",
    "        \n",
    "#         fig.suptitle(f'Partial Dependence of Top 5 Features\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Partial dependence plot creation failed: {str(e)}\")\n",
    "#         print(\"Falling back to feature importance plot.\")\n",
    "        \n",
    "#         # Ensure we have valid feature names for the top 5 features\n",
    "#         valid_indices = [i for i in indices if i < len(feature_names)][:5]\n",
    "#         top_features = [feature_names[i] for i in valid_indices]\n",
    "#         top_importances = importances[valid_indices]\n",
    "\n",
    "#         # Plot feature importances\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         plt.title(f'Feature Importances\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "#         plt.bar(range(len(top_features)), top_importances)\n",
    "#         plt.xticks(range(len(top_features)), top_features, rotation=45, ha='right')\n",
    "#         plt.xlabel('Features')\n",
    "#         plt.ylabel('Importance')\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#     os.makedirs(os.path.join(pca_dir, f'feature_effect_plots/{feature_type}'), exist_ok=True)\n",
    "#     plt.savefig(os.path.join(pca_dir, f'feature_effect_plots/{feature_type}/effect_plot_layer{sae_layer}.png'), dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "#     return top_features\n",
    "\n",
    "# # Modify the train_classifier function to return the trained model and feature names\n",
    "# def train_classifier(X, y, sae_layer, feature_type, pca_dir):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#     rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "#     y_pred = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "#     report = classification_report(y_test, y_pred, target_names=['guess', 'confident'], output_dict=True)\n",
    "#     conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#     importances = rf_classifier.feature_importances_\n",
    "#     feature_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "#     # Plot only top 10 feature importances\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     feature_importances.head(10).plot(kind='bar')\n",
    "#     plt.title(f'Top 10 Feature Importances\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "#     plt.xlabel('Features')\n",
    "#     plt.ylabel('Importance')\n",
    "#     plt.tight_layout()\n",
    "#     os.makedirs(os.path.join(pca_dir, f'classifier_plots/{feature_type}'), exist_ok=True)\n",
    "#     plt.savefig(os.path.join(pca_dir, f'classifier_plots/{feature_type}/feature_importance_layer{sae_layer}.png'), dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "#     return report, conf_matrix, feature_importances, rf_classifier, X_test_scaled, X.columns\n",
    "# def create_activation_heatmap(X, confidence, sae_layer, feature_type, pca_dir):\n",
    "#     # Calculate average activation for guess and confident\n",
    "#     avg_activation = X.groupby(confidence).mean().T\n",
    "    \n",
    "#     # Create heatmap\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     sns.heatmap(avg_activation, cmap='coolwarm', center=0)\n",
    "#     plt.title(f'Average Feature Activation\\nLayer: {sae_layer}, Type: {feature_type}')\n",
    "#     plt.xlabel('Confidence')\n",
    "#     plt.ylabel('Features')\n",
    "    \n",
    "#     # Save heatmap\n",
    "#     os.makedirs(os.path.join(f'activation_heatmaps/{feature_type}'), exist_ok=True)\n",
    "#     plt.savefig(os.path.join(f'activation_heatmaps/{feature_type}/activation_heatmap_layer{sae_layer}.png'), dpi = 300)\n",
    "#     plt.close()\n",
    "    \n",
    "#     return avg_activation\n",
    "\n",
    "# # Create output directories\n",
    "# os.makedirs(os.path.join(pca_dir, 'pca_plots'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(pca_dir, 'classifier_plots'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(pca_dir, 'text_results'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(pca_dir, 'activation_heatmaps'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(pca_dir, 'feature_effect_plots'), exist_ok=True)\n",
    "\n",
    "# pca_data = pca_data[pca_data['sae_layer'] == 21]\n",
    "# # Group by sae_layer and feature_type\n",
    "# grouped = pca_data.groupby(['sae_layer', 'feature_type'])\n",
    "\n",
    "# # Perform analysis for each group\n",
    "# for (sae_layer, feature_type), group in grouped:\n",
    "#     print(f\"Processing Layer: {sae_layer}, Type: {feature_type}\")\n",
    "\n",
    "#     # Prepare data\n",
    "#     X = group.drop(['confidence', 'sae_layer', 'feature_type'], axis=1)\n",
    "#     confidence = group['confidence']\n",
    "#     y = confidence.map({'guess': 0, 'confident': 1})\n",
    "\n",
    "#     # Perform PCA\n",
    "#     loadings = perform_pca(X, confidence, sae_layer, feature_type, pca_dir)\n",
    "\n",
    "#     # Train classifier\n",
    "#     report, conf_matrix, feature_importances, model, X_test, feature_names = train_classifier(X, y, sae_layer, feature_type, pca_dir)\n",
    "\n",
    "#     # Create activation heatmap\n",
    "#     avg_activation = create_activation_heatmap(X, confidence, sae_layer, feature_type, pca_dir)\n",
    "\n",
    "#     # Create feature effect plot\n",
    "#     top_features = create_feature_effect_plot(model, X_test, feature_names, sae_layer, feature_type, pca_dir)\n",
    "\n",
    "#     # Save results\n",
    "#     os.makedirs(os.path.join(pca_dir, f'text_results/{feature_type}'), exist_ok=True)\n",
    "#     with open(os.path.join(pca_dir, f'text_results/{feature_type}/results_layer{sae_layer}.txt'), 'w') as f:\n",
    "#         f.write(f\"Results for Layer: {sae_layer}, Type: {feature_type}\\n\\n\")\n",
    "#         f.write(\"PCA Loadings (top 10):\\n\")\n",
    "#         f.write(loadings['PC1'].abs().sort_values(ascending=False).head(10).to_string())\n",
    "#         f.write(\"\\n\\nClassification Report:\\n\")\n",
    "#         f.write(pd.DataFrame(report).transpose().to_string())\n",
    "#         f.write(\"\\n\\nConfusion Matrix:\\n\")\n",
    "#         f.write(str(conf_matrix))\n",
    "#         f.write(\"\\n\\nTop 10 Feature Importances:\\n\")\n",
    "#         f.write(feature_importances.head(10).to_string())\n",
    "#         f.write(\"\\n\\nAverage Feature Activation:\\n\")\n",
    "#         f.write(avg_activation.to_string())\n",
    "#         f.write(\"\\n\\nTop 5 Features for Partial Dependence Plot:\\n\")\n",
    "#         f.write(\", \".join(map(str, top_features)))  # Ensure all elements are strings\n",
    "\n",
    "# print(\"Analysis complete. Results saved in the 'output' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
